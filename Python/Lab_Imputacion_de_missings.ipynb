{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOb85r+ZKniVRWHvJ+bgCBS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JCaballerot/Tecnicas_de_imputacion_de_datos/blob/main/Python/Lab_Imputacion_de_missings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.ctic.uni.edu.pe/wp-content/uploads/2022/04/588px-x-348px-web-1.png\" alt=\"HTML5 Icon\" width=\"900\" height=\"350\" >"
      ],
      "metadata": {
        "id": "htKJkcZPzR-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<h1 align=center><font size = 5>Técnicas de imputación de datos</font></h1>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Gp9_yiRmzbyZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "R_NIQeNlLo19"
      },
      "source": [
        "## Introducción\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7LRGuxBHMpw"
      },
      "source": [
        "En este laboratorio, aprenderá a usar python para aplicar diversas técnicas de imputación de missings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFB_r2dkHMp7"
      },
      "source": [
        "\n",
        "<h3>Objetivo de este Notebook<h3>    \n",
        "<h5> 1. Como identificar missings en una base de datos.</h5>\n",
        "<h5> 2. Aplicar diersas técnicas de imputación </h5>\n",
        "<h5> 3. Comparar los resultados de imputación aplicadas en un modelo </h5>\n",
        "<h5> 4. Entrenar y Testear un modelo con datos imputados</h5>     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is3nFhGglsft"
      },
      "source": [
        "## Tabla de Contenidos\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<font size = 3>\n",
        "    \n",
        "1. <a href=\"#item31\">Preparación de datos</a>  \n",
        "2. <a href=\"#item32\">Modelo con data completa para comparaciones</a>  \n",
        "3. <a href=\"#item33\">Imputación por medida de tendencia central</a>\n",
        "4. <a href=\"#item33\">Imputación por Regresión</a>\n",
        "5. <a href=\"#item33\">Imputación por KNN</a>  \n",
        "6. <a href=\"#item34\">Imputación por autoencoders</a>  \n",
        "7. <a href=\"#item34\">Imputación por Regresión iterativa</a>  \n",
        "\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preparación de datos"
      ],
      "metadata": {
        "id": "Io9HMl3t0SL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importación de liberías"
      ],
      "metadata": {
        "id": "z9O1fr9H4xlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "rA0QInNE4wA5"
      },
      "execution_count": 1368,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importación del dataset"
      ],
      "metadata": {
        "id": "CHfZofC84iQM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS1clzL0h1fR"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('winequality.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# guardamos el nombre de las variables\n",
        "features = df.columns.tolist()\n",
        "features.remove('quality')\n",
        "target = 'quality'"
      ],
      "metadata": {
        "id": "atQbRuDH-4Hm"
      },
      "execution_count": 1370,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conteo de missings"
      ],
      "metadata": {
        "id": "fRBJn9-w6ATO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contar la cantidad de valores faltantes por columna\n",
        "missing_count = df.isnull().sum()\n",
        "\n",
        "# Obtener la cantidad total de valores faltantes en el conjunto de datos\n",
        "total_missing = missing_count.sum()\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(\"Cantidad de valores faltantes por columna:\")\n",
        "print(missing_count)\n",
        "print(\"Cantidad total de valores faltantes: \", total_missing)"
      ],
      "metadata": {
        "id": "qEPRXvsK5Ay4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Muestreo de datos"
      ],
      "metadata": {
        "id": "Gl9-t0V67ZBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "train_df, test_df = train_test_split(df,\n",
        "                                     test_size = 0.3,\n",
        "                                     random_state = 123)\n",
        "\n",
        "# train_df: conjunto de entrenamiento (80% de los datos)\n",
        "# test_df: conjunto de prueba (20% de los datos)"
      ],
      "metadata": {
        "id": "4cwzhjbN7aqe"
      },
      "execution_count": 1372,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generando Missings random"
      ],
      "metadata": {
        "id": "xCX-jLi76PgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la probabilidad de generar un missing para cada celda\n",
        "probabilidad_missing = 0.3  # Por ejemplo, 20% de probabilidad\n",
        "\n",
        "# Establecer la semilla\n",
        "np.random.seed(123)\n",
        "\n",
        "# Generar valores faltantes de forma aleatoria\n",
        "mask = np.random.rand(*train_df.shape) < probabilidad_missing\n",
        "df_con_missings = train_df.mask(mask)"
      ],
      "metadata": {
        "id": "6w7xrQiu6SMP"
      },
      "execution_count": 1373,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reponemos los valores de la variable objetivo\n",
        "df_con_missings['quality'] = train_df.quality"
      ],
      "metadata": {
        "id": "U-eMNLaG9CEW"
      },
      "execution_count": 1374,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Contar la cantidad de valores faltantes por columna\n",
        "missing_count = df_con_missings.isnull().sum()\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(\"Cantidad de valores faltantes por columna:\")\n",
        "print(missing_count)"
      ],
      "metadata": {
        "id": "0avqlVnp6dJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "univariate = df_con_missings.describe().transpose()\n",
        "univariate['fill_rate'] = univariate['count']/len(df_con_missings)\n",
        "univariate['missing_rate'] = 1 - univariate['fill_rate']\n",
        "\n",
        "univariate"
      ],
      "metadata": {
        "id": "PjhHcaPt8XG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el gráfico de barras\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(missing_count.index, missing_count.values)\n",
        "\n",
        "# Establecer etiquetas y título del gráfico\n",
        "plt.xlabel('Variables')\n",
        "plt.ylabel('Cantidad de missings')\n",
        "plt.title('Missings por variable')\n",
        "\n",
        "# Rotar las etiquetas del eje x si es necesario\n",
        "plt.xticks(rotation='vertical')\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B6b1ZIxi66mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identificamos los missing values visualmente\n",
        "sns.heatmap(df_con_missings.isnull(), cbar=False)\n"
      ],
      "metadata": {
        "id": "Efl94G8kGxIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Modelo con data completa para comparaciones"
      ],
      "metadata": {
        "id": "QzuZcOA--B1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import *"
      ],
      "metadata": {
        "id": "ECYPDygi-Vzy"
      },
      "execution_count": 1379,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el clasificador de árbol de decisión\n",
        "regressor = DecisionTreeRegressor(max_depth = 8, min_samples_leaf = 30)\n",
        "\n",
        "# Entrenar el regresor utilizando los datos de entrenamiento\n",
        "regressor.fit(train_df[features], train_df[target])\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = regressor.predict(test_df[features])\n"
      ],
      "metadata": {
        "id": "uA9UmXW2-YCq"
      },
      "execution_count": 1380,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular métricas de desempeño\n",
        "mse = mean_squared_error(test_df[target], y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(test_df[target], y_pred)\n",
        "mape = np.mean(np.abs((test_df[target] - y_pred) / test_df[target]))*100\n",
        "r2 = np.corrcoef(test_df[target], y_pred)[0, 1]**2\n",
        "\n",
        "dfResults = pd.DataFrame({'metric' : ['MSE', 'RMSE', 'MAE', 'MAPE', 'R2'],\n",
        "              'nomiss' : [mse, rmse, mae, mape,r2]})\n",
        "dfResults"
      ],
      "metadata": {
        "id": "umoR0SvcCKrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Imputación por medida de tendencia central"
      ],
      "metadata": {
        "id": "uXzihwHZCRmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n"
      ],
      "metadata": {
        "id": "BG4eMdYDCitK"
      },
      "execution_count": 1382,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un imputador con estrategia de mediana\n",
        "imputer = SimpleImputer(strategy = 'median')\n",
        "\n",
        "# Imputar los valores faltantes utilizando la mediana\n",
        "imputed_data = imputer.fit_transform(df_con_missings)\n",
        "\n",
        "# Convertir el resultado a un DataFrame\n",
        "imputed_df = pd.DataFrame(imputed_data, columns = df_con_missings.columns.tolist())\n"
      ],
      "metadata": {
        "id": "WJ2obkH8CQxZ"
      },
      "execution_count": 1383,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora entrenaremos el modelo usando la imputación"
      ],
      "metadata": {
        "id": "Ea8w0jC4C3QD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el clasificador de árbol de decisión\n",
        "regressor = DecisionTreeRegressor(max_depth = 8, min_samples_leaf = 30)\n",
        "\n",
        "# Entrenar el regresor utilizando los datos de entrenamiento\n",
        "regressor.fit(imputed_df[features], train_df[target])\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = regressor.predict(test_df[features])\n"
      ],
      "metadata": {
        "id": "9768zNJbCqt5"
      },
      "execution_count": 1384,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular métricas de desempeño\n",
        "mse = mean_squared_error(test_df[target], y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(test_df[target], y_pred)\n",
        "mape = np.mean(np.abs((test_df[target] - y_pred) / test_df[target]))*100\n",
        "r2 = np.corrcoef(test_df[target], y_pred)[0, 1]**2\n",
        "\n",
        "dftemp = pd.DataFrame({'metric' : ['MSE', 'RMSE', 'MAE', 'MAPE', 'R2'],\n",
        "                       'median_imp' : [mse, rmse, mae, mape, r2]})\n",
        "\n",
        "dfResults = pd.merge(dfResults, dftemp)\n",
        "dfResults"
      ],
      "metadata": {
        "id": "tpFP3fA6DXgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Imputación por Regresión"
      ],
      "metadata": {
        "id": "2rZz7BE6FHmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n"
      ],
      "metadata": {
        "id": "5GiEtXVGKZgo"
      },
      "execution_count": 1386,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retirando los missings de la data\n",
        "df_toreg = df_con_missings.copy(deep=True)\n",
        "df_toreg = df_toreg.dropna()\n",
        "\n",
        "# creando una copia del dataset para modificarlo\n",
        "imputed_df = df_con_missings.copy(deep=True)\n",
        "\n",
        "for c in features:\n",
        "  df_tocolreg = df_toreg[list(set(features) - set([c]))]\n",
        "  # Entrenar un modelo de regresión\n",
        "  regressor = LinearRegression()\n",
        "  regressor.fit(df_tocolreg, df_toreg[c])\n",
        "\n",
        "  # Imputar los valores faltantes\n",
        "  df_tofill = df_con_missings[list(set(features) - set([c]))].copy(deep=True)\n",
        "  imputed_df[c + ' pred'] = regressor.predict(df_tofill.fillna(df_tofill.mean()))\n",
        "  imputed_df[c] = imputed_df.apply(lambda row: row[c + ' pred'] if pd.isna(row[c]) else row[c], axis = 1)\n",
        "\n",
        "imputed_df = imputed_df[features]\n"
      ],
      "metadata": {
        "id": "cf6SAmL8IKOm"
      },
      "execution_count": 1387,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora entrenaremos el modelo usando la imputación"
      ],
      "metadata": {
        "id": "i5D7nl1jT_9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el clasificador de árbol de decisión\n",
        "regressor = DecisionTreeRegressor(max_depth = 8, min_samples_leaf = 30)\n",
        "\n",
        "# Entrenar el regresor utilizando los datos de entrenamiento\n",
        "regressor.fit(imputed_df[features], train_df[target])\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = regressor.predict(test_df[features])\n"
      ],
      "metadata": {
        "id": "I0asWmC4T_9z"
      },
      "execution_count": 1388,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular métricas de desempeño\n",
        "mse = mean_squared_error(test_df[target], y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(test_df[target], y_pred)\n",
        "mape = np.mean(np.abs((test_df[target] - y_pred) / test_df[target]))*100\n",
        "r2 = np.corrcoef(test_df[target], y_pred)[0, 1]**2\n",
        "\n",
        "dftemp = pd.DataFrame({'metric' : ['MSE', 'RMSE', 'MAE', 'MAPE', 'R2'],\n",
        "                       'reg_imp' : [mse, rmse, mae, mape, r2]})\n",
        "\n",
        "dfResults = pd.merge(dfResults, dftemp)\n",
        "dfResults"
      ],
      "metadata": {
        "id": "yYkFWHJBT_90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Imputación por KNN"
      ],
      "metadata": {
        "id": "l9C-ItlCVMCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n"
      ],
      "metadata": {
        "id": "_UMiQRiHVr0H"
      },
      "execution_count": 1390,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el objeto KNNImputer\n",
        "knnimputer = KNNImputer(n_neighbors = 30)\n",
        "\n",
        "# Realizar la imputación de missings\n",
        "imputed_df = pd.DataFrame(knnimputer.fit_transform(df_con_missings[features]), columns = features)\n"
      ],
      "metadata": {
        "id": "_DZl1Av3VLs0"
      },
      "execution_count": 1391,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora entrenaremos el modelo usando la imputación"
      ],
      "metadata": {
        "id": "Dng0xW39V2uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el clasificador de árbol de decisión\n",
        "regressor = DecisionTreeRegressor(max_depth = 8, min_samples_leaf = 30)\n",
        "\n",
        "# Entrenar el regresor utilizando los datos de entrenamiento\n",
        "regressor.fit(imputed_df[features], train_df[target])\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = regressor.predict(test_df[features])\n"
      ],
      "metadata": {
        "id": "gERQNAGJV2uB"
      },
      "execution_count": 1392,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular métricas de desempeño\n",
        "mse = mean_squared_error(test_df[target], y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(test_df[target], y_pred)\n",
        "mape = np.mean(np.abs((test_df[target] - y_pred) / test_df[target]))*100\n",
        "r2 = np.corrcoef(test_df[target], y_pred)[0, 1]**2\n",
        "\n",
        "dftemp = pd.DataFrame({'metric' : ['MSE', 'RMSE', 'MAE', 'MAPE', 'R2'],\n",
        "                       'knn_imp' : [mse, rmse, mae, mape, r2]})\n",
        "\n",
        "dfResults = pd.merge(dfResults, dftemp)\n",
        "dfResults"
      ],
      "metadata": {
        "id": "BEKZlxdIV2uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Imputación por Autoencoders"
      ],
      "metadata": {
        "id": "jxG6_RkCZvDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "axnVN5-uZza-"
      },
      "execution_count": 1394,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Escalar los datos en el rango [0, 1]\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df_con_missings[features].fillna(df_con_missings[features].median())), columns=features)\n"
      ],
      "metadata": {
        "id": "OHZYlzunZ0I0"
      },
      "execution_count": 1395,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir el DataFrame en un arreglo numpy\n",
        "data_array = df_scaled.values"
      ],
      "metadata": {
        "id": "4TBHJ2CmaFq2"
      },
      "execution_count": 1396,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un Autoencoder\n",
        "input_dim = len(df_con_missings[features].columns)\n",
        "\n",
        "input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
        "encoded = tf.keras.layers.Dense(encoding_dim, activation='relu')(input_layer)\n",
        "encoded = tf.keras.layers.Dense(100, activation='relu')(input_layer)\n",
        "encoded = tf.keras.layers.Dense(50, activation='relu')(input_layer)\n",
        "encoded = tf.keras.layers.Dense(10, activation='relu')(encoded)\n",
        "decoded = tf.keras.layers.Dense(50, activation='relu')(encoded)\n",
        "decoded = tf.keras.layers.Dense(100, activation='relu')(encoded)\n",
        "decoded = tf.keras.layers.Dense(input_dim, activation='sigmoid')(encoded)\n",
        "\n",
        "autoencoder = tf.keras.models.Model(inputs=input_layer, outputs=decoded)\n"
      ],
      "metadata": {
        "id": "zNht3sLaaJnB"
      },
      "execution_count": 1397,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilar y entrenar el Autoencoder\n",
        "autoencoder.compile(optimizer = tf.keras.optimizers.Adadelta(learning_rate = 0.5, rho = 0.05),\n",
        "                    loss='mean_squared_error')\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience = 5, restore_best_weights = True)\n",
        "\n",
        "autoencoder.fit(data_array, data_array, epochs = 2000, batch_size = 200,\n",
        "                validation_split = 0.3, callbacks = [early_stopping], verbose = 1)\n"
      ],
      "metadata": {
        "id": "iC0479snaW0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener las predicciones del Autoencoder\n",
        "imputed_data_array = autoencoder.predict(data_array)\n",
        "\n",
        "# Convertir las predicciones de vuelta a un DataFrame\n",
        "imputed_df = pd.DataFrame(imputed_data_array, columns=[x + ' pred' for x in features])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD-7BHJqak_r",
        "outputId": "e724c672-a7b3-4287-a2f4-00ece11aead5"
      },
      "execution_count": 1399,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35/35 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando los valores predichos sobre los missings\n",
        "imputed_df[features] = pd.DataFrame(scaler.transform(df_con_missings[features]), columns=features)\n",
        "\n",
        "for c in features:\n",
        "  imputed_df[c] = imputed_df.apply(lambda row: row[c + ' pred'] if pd.isna(row[c]) else row[c], axis = 1)\n",
        "\n",
        "imputed_df = imputed_df[features]"
      ],
      "metadata": {
        "id": "MS46FUYrij6b"
      },
      "execution_count": 1400,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora entrenaremos el modelo usando la imputación"
      ],
      "metadata": {
        "id": "bleo5jahbQvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el clasificador de árbol de decisión\n",
        "regressor = DecisionTreeRegressor(max_depth = 8, min_samples_leaf = 30)\n",
        "\n",
        "# Entrenar el regresor utilizando los datos de entrenamiento\n",
        "regressor.fit(imputed_df[features], train_df[target])\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = regressor.predict(scaler.transform(test_df[features]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drFSiYo_bQvL",
        "outputId": "fa56f7b3-de72-4787-81af-ec7fe178c53b"
      },
      "execution_count": 1401,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular métricas de desempeño\n",
        "mse = mean_squared_error(test_df[target], y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(test_df[target], y_pred)\n",
        "mape = np.mean(np.abs((test_df[target] - y_pred) / test_df[target]))*100\n",
        "r2 = np.corrcoef(test_df[target], y_pred)[0, 1]**2\n",
        "\n",
        "dftemp = pd.DataFrame({'metric' : ['MSE', 'RMSE', 'MAE', 'MAPE', 'R2'],\n",
        "                       'autoencoder_imp' : [mse, rmse, mae, mape, r2]})\n",
        "\n",
        "dfResults = pd.merge(dfResults, dftemp, on = 'metric')\n",
        "dfResults"
      ],
      "metadata": {
        "id": "fePY5zUKbQvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Imputación por Regresión Iterativa"
      ],
      "metadata": {
        "id": "O9iyQoC2t3Yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "M_Apm1fMuBqm"
      },
      "execution_count": 1404,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un objeto IterativeImputer\n",
        "imputer = IterativeImputer(estimator=LinearRegression(), random_state=123)\n",
        "\n",
        "# Imputar los valores missing iterativamente\n",
        "imputed_data = imputer.fit_transform(df_con_missings[features])\n",
        "\n",
        "# Convertir los datos imputados de vuelta a un DataFrame\n",
        "imputed_df = pd.DataFrame(imputed_data, columns=features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i-u4rriuEvH",
        "outputId": "9881e9af-dfc1-4ae8-900e-550bc05bb5bd"
      },
      "execution_count": 1405,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/impute/_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora entrenaremos el modelo usando la imputación"
      ],
      "metadata": {
        "id": "_Jm9Q82buV0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el clasificador de árbol de decisión\n",
        "regressor = DecisionTreeRegressor(max_depth = 8, min_samples_leaf = 30)\n",
        "\n",
        "# Entrenar el regresor utilizando los datos de entrenamiento\n",
        "regressor.fit(imputed_df[features], train_df[target])\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = regressor.predict(test_df[features])\n"
      ],
      "metadata": {
        "id": "Qm8V8eYtuV0W"
      },
      "execution_count": 1406,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular métricas de desempeño\n",
        "mse = mean_squared_error(test_df[target], y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(test_df[target], y_pred)\n",
        "mape = np.mean(np.abs((test_df[target] - y_pred) / test_df[target]))*100\n",
        "r2 = np.corrcoef(test_df[target], y_pred)[0, 1]**2\n",
        "\n",
        "dftemp = pd.DataFrame({'metric' : ['MSE', 'RMSE', 'MAE', 'MAPE', 'R2'],\n",
        "                       'itereg_imp' : [mse, rmse, mae, mape, r2]})\n",
        "\n",
        "dfResults = pd.merge(dfResults, dftemp, on = 'metric')\n",
        "dfResults"
      ],
      "metadata": {
        "id": "0fd249YJuV0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Gracias por completar este laboratorio!\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "9Xd4rJPdy5Sc"
      }
    }
  ]
}